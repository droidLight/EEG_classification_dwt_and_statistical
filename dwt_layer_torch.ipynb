{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dwt_layer_torch",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja_X7cxI1qqg",
        "outputId": "f996a184-d542-479d-c7a0-ab3d9f1e3a9d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pywt\n",
        "from pywt import wavedec\n",
        "import os\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Parameter, Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, Conv1d, MaxPool2d, MaxPool1d,AvgPool2d, Module, Softmax, BatchNorm2d, Dropout2d, Sigmoid, BCEWithLogitsLoss, LeakyReLU, BatchNorm1d, PReLU, Dropout, BCELoss, LogSoftmax, NLLLoss\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, MultiplicativeLR\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.signal import butter, lfilter, iirnotch, filtfilt, sosfilt\n",
        "import shutil\n",
        "from scipy import signal\n",
        "import subprocess as sp\n",
        "import time\n",
        "seed = 3\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed = seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNxxF0kgUV_0",
        "outputId": "0f814871-a2a0-48b7-bef9-7e8e57ead846"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/My Drive/chb_dataset/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH_xXzhjvED4"
      },
      "source": [
        "def apply_filter(segment):\n",
        "  b, a = iirnotch(60, 20, 256)\n",
        "  noise_removed = lfilter(b, a, segment)\n",
        "\n",
        "  b, a = iirnotch(50, 16, 256)\n",
        "  noise_removed = lfilter(b, a, noise_removed)\n",
        "\n",
        "  low = 0.5 / (0.5 * 256)\n",
        "  high = 40 / (0.5 * 256)\n",
        "  \n",
        "  sos = butter(6, [low, high], btype = \"bandpass\", output = 'sos')\n",
        "  filtered = sosfilt(sos, noise_removed)\n",
        "  return filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_m-1wHnUjkM"
      },
      "source": [
        "def list_to_data(data_list, label, window_size, overlap, downsample = 0):\n",
        "    x, y = list(), list()\n",
        "    input_interval = 256 * window_size\n",
        "    inc = int(input_interval / overlap)\n",
        "    for data in data_list:\n",
        "      for i in range(0, data.shape[1], inc):\n",
        "          end_input = i + input_interval\n",
        "\n",
        "          if end_input < data.shape[1]:\n",
        "            img_seg = None\n",
        "            if downsample != 0:              \n",
        "              img_seg = apply_filter(data[:, i : end_input:downsample])\n",
        "\n",
        "            else:              \n",
        "              img_seg = apply_filter(data[:, i : end_input])\n",
        "\n",
        "            x.append(img_seg)\n",
        "            y.append(label)\n",
        "\n",
        "    x, y = np.array(x), np.array(y)\n",
        "    x, y = shuffle(x, y, random_state = seed)\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2pPh-4QUlI6"
      },
      "source": [
        "#new\n",
        "def save_data(window_size, x, y, start_value = 0, del_dir = False):\n",
        "  count = start_value\n",
        "  x_list, y_list = list(), list()\n",
        "  if del_dir:\n",
        "    try:\n",
        "      shutil.rmtree('my_data')\n",
        "    except:\n",
        "      pass\n",
        "  if not os.path.exists('my_data'):\n",
        "    os.makedirs('my_data')\n",
        "\n",
        "  for x_temp, y_temp in zip(x, y):\n",
        "    file_name = \"my_data/x_\"+str(window_size)+\"_\"+str(count)+\".npy\"\n",
        "    np.save(file_name, x_temp)\n",
        "    x_list.append(file_name)\n",
        "    y_list.append(y_temp)\n",
        "    count += 1\n",
        "\n",
        "  data = pd.DataFrame()\n",
        "  data['file'] = x_list\n",
        "  data['label'] = y_list\n",
        "  return data\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVuMjYmtU0F-"
      },
      "source": [
        "def create_list(pre_data, inter_data, window_size, overlap, downsample = 0, start_value = 0):\n",
        "  x, y = list_to_data(pre_data, 1, window_size, overlap, downsample)\n",
        "  pre_list = save_data(window_size, x, y, start_value)\n",
        "  del x\n",
        "  del y\n",
        "\n",
        "  x, y = list_to_data(inter_data, 0, window_size, overlap, downsample)\n",
        "  inter_list = save_data(window_size, x, y, pre_list.shape[0])\n",
        "  del x\n",
        "  del y\n",
        "\n",
        "  train_size = int(0.8 * pre_list.shape[0])\n",
        "  val_size = int(0.9 * pre_list.shape[0])\n",
        "\n",
        "  train_list = pd.concat([pre_list[0:train_size], inter_list[0:train_size]])\n",
        "\n",
        "  val_list = pd.concat([pre_list[train_size:val_size], inter_list[train_size:val_size]])\n",
        "\n",
        "  test_list = pd.concat([pre_list[val_size:], inter_list[val_size:]])\n",
        "  \n",
        "  return train_list, val_list, test_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiU_Mo4so5dE",
        "outputId": "513451b0-cd59-41ed-a220-1c40d8c85416"
      },
      "source": [
        "patient_num = 6\n",
        "data_path = drive_path + 'processed_data/'\n",
        "result_path = drive_path + 'new_results/patient_'+str(patient_num)+'/'\n",
        "PREICTAL_INTERVAL_IN_MINS = 20\n",
        "WINDOW_SIZE = 10\n",
        "OVERLAP = 6\n",
        "IS_DWT = True\n",
        "IS_CONT = True\n",
        "WINDOW_SIZE_END = 40\n",
        "model_file = \"new_p\"+str(patient_num)+\"_\"+(\"dwt_\" if IS_DWT else \"\")+str(PREICTAL_INTERVAL_IN_MINS)+\"_\"+('cont_'+str(WINDOW_SIZE)+\"_\"+str(WINDOW_SIZE_END) if IS_CONT else str(WINDOW_SIZE))+\"_\"+str(OVERLAP)+\".pt\"\n",
        "train_history_file = \"new_p\"+str(patient_num)+\"_\"+(\"dwt_\" if IS_DWT else \"\")+str(PREICTAL_INTERVAL_IN_MINS)+\"_\"+('cont_'+str(WINDOW_SIZE)+\"_\"+str(WINDOW_SIZE_END) if IS_CONT else str(WINDOW_SIZE))+\"_\"+str(OVERLAP)+\".csv\"\n",
        "log_file = \"new_p\"+str(patient_num)+\"_\"+(\"dwt_\" if IS_DWT else \"\")+str(PREICTAL_INTERVAL_IN_MINS)+\"_\"+('cont_'+str(WINDOW_SIZE)+\"_\"+str(WINDOW_SIZE_END) if IS_CONT else str(WINDOW_SIZE))+\"_\"+str(OVERLAP)+\".txt\"\n",
        "plot_file = \"new_p\"+str(patient_num)+\"_\"+(\"dwt_\" if IS_DWT else \"\")+str(PREICTAL_INTERVAL_IN_MINS)+\"_\"+('cont_'+str(WINDOW_SIZE)+\"_\"+str(WINDOW_SIZE_END) if IS_CONT else str(WINDOW_SIZE))+\"_\"+str(OVERLAP)+\".png\"\n",
        "\n",
        "if not os.path.exists(result_path):\n",
        "  os.makedirs(result_path)\n",
        "\n",
        "print(f\"Model filename: {model_file}\")\n",
        "print(f\"Train history file: {train_history_file}\")\n",
        "print(f\"Log  file: {log_file}\")\n",
        "print(f\"Plot file: {plot_file}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model filename: new_p6_dwt_20_cont_10_40_6.pt\n",
            "Train history file: new_p6_dwt_20_cont_10_40_6.csv\n",
            "Log  file: new_p6_dwt_20_cont_10_40_6.txt\n",
            "Plot file: new_p6_dwt_20_cont_10_40_6.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUoX24n2mEWS"
      },
      "source": [
        "inter_name = f\"patient_{patient_num}_inter_interval_{PREICTAL_INTERVAL_IN_MINS}.npy\"\n",
        "pre_name = f\"patient_{patient_num}_pre_interval_{PREICTAL_INTERVAL_IN_MINS}.npy\"\n",
        "\n",
        "pre = np.load(data_path + pre_name)\n",
        "inter = np.load(data_path + inter_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-GVo13Jnugy"
      },
      "source": [
        "\n",
        "Network with trainable dwt parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpAzAOyLszIE",
        "outputId": "7d98fafa-1b47-49e1-e49f-fd072f1f4d2c"
      },
      "source": [
        "#rough\n",
        "x = torch.randn((32, 18, 1289))\n",
        "conv_1 = Conv1d(18, 12, kernel_size = 3, stride = 1)\n",
        "conv_2 = Conv1d(12, 6, kernel_size = 3, stride = 2)\n",
        "conv_3 = Conv1d(6, 3, kernel_size = 5, stride = 1)\n",
        "conv_4 = Conv1d(3, 1, kernel_size = 5, stride = 2)\n",
        "maxpool = MaxPool1d(2, stride = 1)\n",
        "\n",
        "\n",
        "x = F.relu(conv_1(x))\n",
        "x = conv_2(x)\n",
        "x = maxpool(x)\n",
        "print(f\"After block one: {x.size()}\")\n",
        "x = conv_3(x)\n",
        "x = conv_4(x)\n",
        "#x = maxpool(x)\n",
        "print(f\"After block two: {x.size()}\")\n",
        "#x = conv_5(x)\n",
        "#x = conv_6(x)\n",
        "print(f\"Final size: {x.size()}\")\n",
        "\n",
        "\n",
        "cd1 = torch.randn(x.size()).squeeze(1)\n",
        "cd2 = torch.randn(x.size()).squeeze(1)\n",
        "cd3 = torch.randn(x.size()).squeeze(1)\n",
        "cd4 = torch.randn(x.size()).squeeze(1)\n",
        "print(cd1.size())\n",
        "result = torch.cat([cd1, cd2, cd3, cd4], dim = 1)\n",
        "print(result.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After block one: torch.Size([32, 6, 642])\n",
            "After block two: torch.Size([32, 1, 317])\n",
            "Final size: torch.Size([32, 1, 317])\n",
            "torch.Size([32, 317])\n",
            "torch.Size([32, 1268])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L6GR5wbnYFs"
      },
      "source": [
        "class CoeffNet(Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CoeffNet, self).__init__()\n",
        "\n",
        "    self.conv_1 = Conv1d(18, 15, kernel_size = 3, stride = 2)\n",
        "    self.conv_2 = Conv1d(15, 12, kernel_size = 3, stride = 2)\n",
        "    self.conv_3 = Conv1d(12, 9, kernel_size = 5, stride = 1)\n",
        "    self.conv_4 = Conv1d(9, 6, kernel_size = 5, stride = 1)\n",
        "    self.conv_5 = Conv1d(6, 3, kernel_size = 7, stride = 1)\n",
        "    self.conv_6 = Conv1d(3, 1, kernel_size = 7, stride = 1)\n",
        "    '''\n",
        "    self.conv_1 = Conv1d(18, 12, kernel_size = 3, stride = 1)\n",
        "    self.conv_2 = Conv1d(12, 6, kernel_size = 3, stride = 2)\n",
        "    self.conv_3 = Conv1d(6, 3, kernel_size = 5, stride = 1)\n",
        "    self.conv_4 = Conv1d(3, 1, kernel_size = 5, stride = 2)\n",
        "    '''\n",
        "    self.maxpool = MaxPool1d(2, stride = 1)\n",
        "    self.batchNorm_1 = BatchNorm1d(12)\n",
        "    self.batchNorm_2 = BatchNorm1d(6)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = F.relu(self.conv_1(x))\n",
        "    x = F.relu(self.conv_2(x))\n",
        "    x = self.batchNorm_1(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    x = F.relu(self.conv_3(x))\n",
        "    x = F.relu(self.conv_4(x))\n",
        "    x = self.batchNorm_2(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    x = F.relu(self.conv_5(x))\n",
        "    x = F.relu(self.conv_6(x))\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uJu10KQnphi"
      },
      "source": [
        "#trainable parameter neural network\n",
        "class CustomDWT(Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CustomDWT, self).__init__()    \n",
        "    \n",
        "    self.batchNorm = BatchNorm1d(512)\n",
        "\n",
        "    self.linear_1 = Linear(1196, 1024)#1196\n",
        "    self.linear_2 = Linear(1024, 512)\n",
        "    self.linear_3 = Linear(512, 256)\n",
        "    self.linear_4 = Linear(256, 2)\n",
        "\n",
        "    \n",
        "    self.cd1_branch = CoeffNet()\n",
        "    self.cd2_branch = CoeffNet()\n",
        "    self.cd3_branch = CoeffNet()\n",
        "    self.cd4_branch = CoeffNet()\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.cpu().detach().numpy()\n",
        "    cd1, cd2, cd3, cd4 = list(), list(), list(), list()\n",
        "\n",
        "    #iterating on each sample inside a batch\n",
        "    for i in range(x.shape[0]): \n",
        "      item = x[i]            \n",
        "      cd1_temp, cd2_temp, cd3_temp, cd4_temp = list(), list(), list(), list()\n",
        "\n",
        "      #iterating each channel\n",
        "      for ch in range(item.shape[0]):\n",
        "        single_ch = item[ch]\n",
        "        d_and_a = wavedec(single_ch, 'db10', level = 4)\n",
        "        del d_and_a[0]\n",
        "        max_length = max(len(d_and_a[0]), len(d_and_a[1]), len(d_and_a[2]), len(d_and_a[3]))      \n",
        "\n",
        "        for coeff_index in range(4):\n",
        "          d_and_a[coeff_index] = np.concatenate([d_and_a[coeff_index], [0 for l in range(max_length - len(d_and_a[coeff_index]))]])\n",
        "        \n",
        "        cd1_temp.append(d_and_a[0])\n",
        "        cd2_temp.append(d_and_a[1])\n",
        "        cd3_temp.append(d_and_a[2])\n",
        "        cd4_temp.append(d_and_a[3])\n",
        "      \n",
        "      cd1.append(cd1_temp)\n",
        "      cd2.append(cd2_temp)\n",
        "      cd3.append(cd3_temp)\n",
        "      cd4.append(cd4_temp)\n",
        "    \n",
        "    cd1 = torch.from_numpy(np.array(cd1)).to(device)\n",
        "    cd2 = torch.from_numpy(np.array(cd2)).to(device)\n",
        "    cd3 = torch.from_numpy(np.array(cd3)).to(device)\n",
        "    cd4 = torch.from_numpy(np.array(cd4)).to(device)\n",
        "    \n",
        "    cd1 = self.cd1_branch(cd1).squeeze(1)\n",
        "    cd2 = self.cd2_branch(cd2).squeeze(1)\n",
        "    cd3 = self.cd3_branch(cd3).squeeze(1)\n",
        "    cd4 = self.cd4_branch(cd4).squeeze(1)\n",
        "    \n",
        "\n",
        "    x = torch.cat([cd1, cd2, cd3, cd4], dim = 1).to(device)\n",
        "    x = F.relu(self.linear_1(x))\n",
        "    x = F.relu(self.linear_2(x))\n",
        "    x = self.batchNorm(x)\n",
        "    x = F.relu(self.linear_3(x))\n",
        "    x = F.relu(self.linear_4(x))\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ_WIR-oKv9J"
      },
      "source": [
        "#from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta= 0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def is_best(self, val_loss):\n",
        "      score = -val_loss\n",
        "      \n",
        "      if self.best_score is None:\n",
        "        self.best_score = score\n",
        "\n",
        "      elif score < self.best_score + self.delta:\n",
        "        return False\n",
        "      else:\n",
        "        return True\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Ux2abeSBYf"
      },
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "MOMENTUM = 0.6\n",
        "L2 = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_qjgA08Fhuy"
      },
      "source": [
        "model = CustomDWT()\n",
        "criterion = CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "model.double()\n",
        "criterion = criterion.to(device)\n",
        "optimizer = SGD(model.parameters(), lr = LEARNING_RATE, momentum = MOMENTUM, weight_decay = L2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCt7Lg2XLfzm"
      },
      "source": [
        "class PytorchDataGen(Dataset):\n",
        "\n",
        "  def __init__(self, data):\n",
        "    self.file_list = data['file'].values.tolist()\n",
        "    self.labels = data['label'].values.tolist()\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.file_list)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    file_name = self.file_list[index]\n",
        "    x = np.load(file_name)    \n",
        "    y = self.labels[index]\n",
        "    return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJBS6OqCoWiY"
      },
      "source": [
        "!rm -rf my_data\n",
        "!rm -rf checkpoint.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri4A9riahAfc"
      },
      "source": [
        "train_list, val_list, test_list = create_list(pre, inter, WINDOW_SIZE, OVERLAP, downsample = (WINDOW_SIZE // 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzy3silScer3",
        "outputId": "24f62e1f-28ba-4119-c58d-03dbbb92ab46"
      },
      "source": [
        "t = np.load(train_list['file'][0].values[0])\n",
        "print(t.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18, 2560)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTlxDmtqcoxw"
      },
      "source": [
        "'''\n",
        "ONLY FOR CONT LEARNING\n",
        "Process data given list of window sizes\n",
        "'''\n",
        "window_sizes = [i for i in range(10, WINDOW_SIZE_END + 10, 10)]\n",
        "train_list = pd.DataFrame()\n",
        "val_list = pd.DataFrame()\n",
        "test_list = pd.DataFrame()\n",
        "\n",
        "for window_size in window_sizes:\n",
        "  train_list_temp, val_list_temp, test_list_temp = create_list(pre, inter, window_size, OVERLAP, downsample = (window_size // 10))\n",
        "  train_list = pd.concat([train_list, train_list_temp], axis = 0).reset_index(drop = True)\n",
        "  val_list = pd.concat([val_list, val_list_temp], axis = 0).reset_index(drop = True)\n",
        "  test_list = pd.concat([test_list, test_list_temp], axis = 0).reset_index(drop = True)\n",
        "\n",
        "train_list = train_list.sample(frac = 1).reset_index(drop = True)\n",
        "val_list = val_list.sample(frac = 1).reset_index(drop = True)\n",
        "test_list = test_list.sample(frac = 1).reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3xtHnRu6dZX",
        "outputId": "e3d6b668-3bec-4695-dd20-eafbff9e2c32"
      },
      "source": [
        "print(f\"Train list: {train_list.shape} Val list: {val_list.shape} Test list: {test_list.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train list: (18942, 2) Val list: (2368, 2) Test list: (2370, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot--BliEmyDB"
      },
      "source": [
        "batch_size = 32\n",
        "train_params = {\n",
        "    'batch_size':batch_size,\n",
        "    'shuffle':True,\n",
        "    'num_workers':2\n",
        "}\n",
        "val_params = {\n",
        "    'batch_size':batch_size,\n",
        "    'shuffle':False,\n",
        "    'num_workers':2\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV_Of-Ujotmt"
      },
      "source": [
        "train_data = PytorchDataGen(train_list)\n",
        "val_data = PytorchDataGen(val_list)\n",
        "\n",
        "train_gen = DataLoader(train_data, **train_params, drop_last = True)\n",
        "val_gen = DataLoader(val_data, **val_params, drop_last = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA4_8rqopYaF"
      },
      "source": [
        "#for using with crossentropy loss\n",
        "def calc_acc(y_pred, y_true):\n",
        "  prob = F.softmax(y_pred, dim = 1)\n",
        "  pred_label = np.argmax(prob.detach().cpu().numpy(), axis = 1)\n",
        "  y_true = y_true.detach().cpu().numpy()\n",
        "  acc = (pred_label == y_true).sum()/pred_label.shape[0]\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bWID3FfxQOT"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQcrAA5_tiuc",
        "outputId": "9caa3f35-27e7-429d-a8f5-7f1e725f3310"
      },
      "source": [
        "early_stopping = EarlyStopping(patience = 20)\n",
        "EPOCHS = 100\n",
        "train_loss_list, val_loss_list = list(), list()\n",
        "train_acc_list, val_acc_list = list(), list()\n",
        "early_stop_epoch = 0\n",
        "\n",
        "train_best_loss = 0\n",
        "val_best_loss = 0\n",
        "train_best_acc = 0\n",
        "val_best_acc = 0\n",
        "lr_list = []\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  train_running_loss, train_running_acc = list(), list()\n",
        "  val_running_loss, val_running_acc = list(), list()\n",
        "  #training loop\n",
        "  model.train()  \n",
        "  for (x_train, y_train) in train_gen:\n",
        "    x_train = x_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(x_train)  \n",
        "    train_loss_func = criterion(pred, y_train)\n",
        "    train_loss_func.backward()\n",
        "    optimizer.step()\n",
        "        \n",
        "    acc = calc_acc(pred, y_train)\n",
        "    train_running_acc.append(acc)\n",
        "    train_running_loss.append(train_loss_func.item())\n",
        "    \n",
        "  train_loss = np.average(train_running_loss)\n",
        "  train_acc = np.average(train_running_acc)\n",
        "  train_acc_list.append(train_acc)\n",
        "  train_loss_list.append(train_loss)\n",
        "\n",
        "\n",
        "  #validation loop\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for (x_val, y_val) in val_gen:\n",
        "      x_val = x_val.to(device)\n",
        "      y_val = y_val.to(device)\n",
        "  \n",
        "      pred = model(x_val)\n",
        "      val_loss_func = criterion(pred, y_val)\n",
        "      acc = calc_acc(pred, y_val)\n",
        "      val_running_acc.append(acc)\n",
        "      val_running_loss.append(val_loss_func.item())\n",
        "    \n",
        "    val_loss = np.average(val_running_loss)\n",
        "    val_acc = np.average(val_running_acc)\n",
        "    val_acc_list.append(val_acc)\n",
        "    val_loss_list.append(val_loss)\n",
        "  \n",
        "    \n",
        "  print(f\"Epoch: {epoch + 1}/{EPOCHS}   Train Acc = {train_acc}  Train Loss = {train_loss}    Val Acc = {val_acc}  Val Loss = {val_loss}\")  \n",
        "  \n",
        "  #Earlystopping\n",
        "  early_stopping(val_loss, model)\n",
        "\n",
        "  if early_stopping.is_best(val_loss):\n",
        "    early_stop_epoch = epoch\n",
        "    train_best_loss = train_loss\n",
        "    train_best_acc = train_acc\n",
        "    val_best_loss = val_loss\n",
        "    val_best_acc = val_acc\n",
        "  \n",
        "  if early_stopping.early_stop:\n",
        "    print(\"EARLY STOPPING\")\n",
        "    break\n",
        "\n",
        "    \n",
        "print(f\"Train loss: {train_best_loss}\")\n",
        "print(f\"Train acc: {train_best_acc}\")\n",
        "print(f\"Val loss: {val_best_loss}\")\n",
        "print(f\"Val acc: {val_best_acc}\")\n",
        "print(f\"Time taken: {time.time() - start_time}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/100   Train Acc = 0.49746192893401014  Train Loss = 0.697828190049869    Val Acc = 0.5206925675675675  Val Loss = 0.6933083144989904\n",
            "Epoch: 2/100   Train Acc = 0.5377009306260575  Train Loss = 0.6823935020244912    Val Acc = 0.5388513513513513  Val Loss = 0.6647295040611558\n",
            "Epoch: 3/100   Train Acc = 0.5864530456852792  Train Loss = 0.6427568552915937    Val Acc = 0.6258445945945946  Val Loss = 0.6178596454894187\n",
            "Epoch: 4/100   Train Acc = 0.6692047377326565  Train Loss = 0.6033156912273057    Val Acc = 0.7115709459459459  Val Loss = 0.5763694200096655\n",
            "Epoch: 5/100   Train Acc = 0.7214467005076142  Train Loss = 0.5734840363684802    Val Acc = 0.7508445945945946  Val Loss = 0.5482316085347244\n",
            "Epoch: 6/100   Train Acc = 0.7384200507614214  Train Loss = 0.5527318068671767    Val Acc = 0.7584459459459459  Val Loss = 0.5305489069291696\n",
            "Epoch: 7/100   Train Acc = 0.7462986463620981  Train Loss = 0.540079623016015    Val Acc = 0.7668918918918919  Val Loss = 0.5199109560117997\n",
            "Epoch: 8/100   Train Acc = 0.7521150592216582  Train Loss = 0.530658259682332    Val Acc = 0.7673141891891891  Val Loss = 0.5141073683021009\n",
            "Epoch: 9/100   Train Acc = 0.7533840947546532  Train Loss = 0.526438068244379    Val Acc = 0.7715371621621622  Val Loss = 0.5088836492222305\n",
            "Epoch: 10/100   Train Acc = 0.7571912013536379  Train Loss = 0.5192910839971192    Val Acc = 0.769847972972973  Val Loss = 0.5019257964892762\n",
            "Epoch: 11/100   Train Acc = 0.7585659898477157  Train Loss = 0.5141325157007804    Val Acc = 0.7753378378378378  Val Loss = 0.5000040594014724\n",
            "Epoch: 12/100   Train Acc = 0.7571912013536379  Train Loss = 0.5088712766215243    Val Acc = 0.7711148648648649  Val Loss = 0.49346746842213957\n",
            "Epoch: 13/100   Train Acc = 0.7604166666666666  Train Loss = 0.5050928855779162    Val Acc = 0.7706925675675675  Val Loss = 0.48896091393969177\n",
            "Epoch: 14/100   Train Acc = 0.7622673434856176  Train Loss = 0.5023001755202382    Val Acc = 0.768581081081081  Val Loss = 0.48594893966046343\n",
            "Epoch: 15/100   Train Acc = 0.7617914551607445  Train Loss = 0.49838815144190957    Val Acc = 0.7706925675675675  Val Loss = 0.483046055095093\n",
            "Epoch: 16/100   Train Acc = 0.7644352791878173  Train Loss = 0.49506531584779045    Val Acc = 0.7690033783783784  Val Loss = 0.4807494689881281\n",
            "Epoch: 17/100   Train Acc = 0.7649640439932318  Train Loss = 0.4908735800097204    Val Acc = 0.7732263513513513  Val Loss = 0.4783760128744812\n",
            "Epoch: 18/100   Train Acc = 0.7680837563451777  Train Loss = 0.48805205453906014    Val Acc = 0.7740709459459459  Val Loss = 0.47567171495887434\n",
            "Epoch: 19/100   Train Acc = 0.7666560913705583  Train Loss = 0.48903040101122713    Val Acc = 0.7728040540540541  Val Loss = 0.4746000522945211\n",
            "Epoch: 20/100   Train Acc = 0.768348138747885  Train Loss = 0.48261867659044255    Val Acc = 0.7732263513513513  Val Loss = 0.47264593626656515\n",
            "Epoch: 21/100   Train Acc = 0.7684538917089678  Train Loss = 0.48176314823215155    Val Acc = 0.7778716216216216  Val Loss = 0.4692487253664655\n",
            "Epoch: 22/100   Train Acc = 0.7702516920473773  Train Loss = 0.47957681252445816    Val Acc = 0.7744932432432432  Val Loss = 0.4714809552154035\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 23/100   Train Acc = 0.7721552453468697  Train Loss = 0.47619480666446173    Val Acc = 0.778293918918919  Val Loss = 0.4684845114675756\n",
            "Epoch: 24/100   Train Acc = 0.7725253807106599  Train Loss = 0.4753129049408781    Val Acc = 0.7774493243243243  Val Loss = 0.46568390052494907\n",
            "Epoch: 25/100   Train Acc = 0.7748519458544839  Train Loss = 0.4722008213953831    Val Acc = 0.7774493243243243  Val Loss = 0.4665775441033984\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 26/100   Train Acc = 0.7740059221658206  Train Loss = 0.4714216894566576    Val Acc = 0.7799831081081081  Val Loss = 0.46017650584345715\n",
            "Epoch: 27/100   Train Acc = 0.7763324873096447  Train Loss = 0.46782480260574316    Val Acc = 0.7816722972972973  Val Loss = 0.45979500465516376\n",
            "Epoch: 28/100   Train Acc = 0.7798223350253807  Train Loss = 0.46499442718126327    Val Acc = 0.7829391891891891  Val Loss = 0.458431310018954\n",
            "Epoch: 29/100   Train Acc = 0.781355752961083  Train Loss = 0.4628488595939535    Val Acc = 0.7795608108108109  Val Loss = 0.4570513635423752\n",
            "Epoch: 30/100   Train Acc = 0.7816730118443317  Train Loss = 0.46128504716621954    Val Acc = 0.7761824324324325  Val Loss = 0.4614160703484991\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 31/100   Train Acc = 0.7846869712351946  Train Loss = 0.4575590275159536    Val Acc = 0.78125  Val Loss = 0.4548809744268887\n",
            "Epoch: 32/100   Train Acc = 0.7890757191201354  Train Loss = 0.4567892611878981    Val Acc = 0.7804054054054054  Val Loss = 0.4554239393805662\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 33/100   Train Acc = 0.7858502538071066  Train Loss = 0.45533279444554764    Val Acc = 0.7833614864864865  Val Loss = 0.45323289310970677\n",
            "Epoch: 34/100   Train Acc = 0.7890228426395939  Train Loss = 0.4542078829545447    Val Acc = 0.7820945945945946  Val Loss = 0.45147843730002163\n",
            "Epoch: 35/100   Train Acc = 0.7898159898477157  Train Loss = 0.4513577085081182    Val Acc = 0.7850506756756757  Val Loss = 0.44876095270255467\n",
            "Epoch: 36/100   Train Acc = 0.7903976311336718  Train Loss = 0.44816606915990886    Val Acc = 0.7888513513513513  Val Loss = 0.44724941080472735\n",
            "Epoch: 37/100   Train Acc = 0.7917724196277496  Train Loss = 0.4476598270516074    Val Acc = 0.7858952702702703  Val Loss = 0.44827773054308706\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 38/100   Train Acc = 0.79542089678511  Train Loss = 0.44388829146572195    Val Acc = 0.7896959459459459  Val Loss = 0.4444754315295456\n",
            "Epoch: 39/100   Train Acc = 0.7956324027072758  Train Loss = 0.4439037837646133    Val Acc = 0.7850506756756757  Val Loss = 0.44380564946938134\n",
            "Epoch: 40/100   Train Acc = 0.796002538071066  Train Loss = 0.44033787125431106    Val Acc = 0.7913851351351351  Val Loss = 0.44281260679000656\n",
            "Epoch: 41/100   Train Acc = 0.7993337563451777  Train Loss = 0.43964440914519715    Val Acc = 0.7892736486486487  Val Loss = 0.4421344194925699\n",
            "Epoch: 42/100   Train Acc = 0.8009200507614214  Train Loss = 0.4366334060784803    Val Acc = 0.784206081081081  Val Loss = 0.44551726005336934\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 43/100   Train Acc = 0.8012901861252115  Train Loss = 0.4345304793932498    Val Acc = 0.7880067567567568  Val Loss = 0.4432678815593947\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 44/100   Train Acc = 0.800655668358714  Train Loss = 0.43384069492242167    Val Acc = 0.7930743243243243  Val Loss = 0.43920175344948525\n",
            "Epoch: 45/100   Train Acc = 0.8026649746192893  Train Loss = 0.42947451102334305    Val Acc = 0.7880067567567568  Val Loss = 0.4397506170508527\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 46/100   Train Acc = 0.8056789340101523  Train Loss = 0.43117766422327675    Val Acc = 0.7871621621621622  Val Loss = 0.44526576529188694\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 47/100   Train Acc = 0.8047800338409475  Train Loss = 0.4262774897812831    Val Acc = 0.7913851351351351  Val Loss = 0.436609833750483\n",
            "Epoch: 48/100   Train Acc = 0.8060490693739425  Train Loss = 0.42621244688294235    Val Acc = 0.7863175675675675  Val Loss = 0.4484795971702947\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 49/100   Train Acc = 0.8074238578680203  Train Loss = 0.42536894737141795    Val Acc = 0.792652027027027  Val Loss = 0.43863293260997216\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 50/100   Train Acc = 0.8069479695431472  Train Loss = 0.4240489252726769    Val Acc = 0.7875844594594594  Val Loss = 0.4441034016732338\n",
            "EarlyStopping counter: 3 out of 20\n",
            "Epoch: 51/100   Train Acc = 0.8138219120135364  Train Loss = 0.4196130309813294    Val Acc = 0.7888513513513513  Val Loss = 0.43863882839275103\n",
            "EarlyStopping counter: 4 out of 20\n",
            "Epoch: 52/100   Train Acc = 0.8121827411167513  Train Loss = 0.4178787702996118    Val Acc = 0.7985641891891891  Val Loss = 0.4322122450390024\n",
            "Epoch: 53/100   Train Acc = 0.8139805414551607  Train Loss = 0.4171590532386061    Val Acc = 0.8002533783783784  Val Loss = 0.4307533837493188\n",
            "Epoch: 54/100   Train Acc = 0.8134517766497462  Train Loss = 0.41328842080020745    Val Acc = 0.8002533783783784  Val Loss = 0.43034646555314565\n",
            "Epoch: 55/100   Train Acc = 0.8160427241962775  Train Loss = 0.4129611501296668    Val Acc = 0.7913851351351351  Val Loss = 0.43418774113480846\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 56/100   Train Acc = 0.8164128595600677  Train Loss = 0.40960084228931626    Val Acc = 0.7956081081081081  Val Loss = 0.43273465368245184\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 57/100   Train Acc = 0.8195325719120136  Train Loss = 0.4076874497863769    Val Acc = 0.7989864864864865  Val Loss = 0.4286686244811848\n",
            "Epoch: 58/100   Train Acc = 0.8164128595600677  Train Loss = 0.40940090219888486    Val Acc = 0.8044763513513513  Val Loss = 0.42714174028790375\n",
            "Epoch: 59/100   Train Acc = 0.8236569373942471  Train Loss = 0.4036955646673625    Val Acc = 0.8074324324324325  Val Loss = 0.4275461220369666\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 60/100   Train Acc = 0.824238578680203  Train Loss = 0.40299400513667155    Val Acc = 0.7994087837837838  Val Loss = 0.4277243308350848\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 61/100   Train Acc = 0.8254547377326565  Train Loss = 0.39941933411528785    Val Acc = 0.7981418918918919  Val Loss = 0.4284971094277054\n",
            "EarlyStopping counter: 3 out of 20\n",
            "Epoch: 62/100   Train Acc = 0.825507614213198  Train Loss = 0.398978648736302    Val Acc = 0.7977195945945946  Val Loss = 0.43133879470985587\n",
            "EarlyStopping counter: 4 out of 20\n",
            "Epoch: 63/100   Train Acc = 0.8249788494077834  Train Loss = 0.39795484882669657    Val Acc = 0.8006756756756757  Val Loss = 0.426546496351038\n",
            "Epoch: 64/100   Train Acc = 0.8277813028764806  Train Loss = 0.39361024852110704    Val Acc = 0.7989864864864865  Val Loss = 0.42573452203858253\n",
            "Epoch: 65/100   Train Acc = 0.8302136209813875  Train Loss = 0.3926466741476035    Val Acc = 0.7977195945945946  Val Loss = 0.43048009832966827\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 66/100   Train Acc = 0.8309538917089678  Train Loss = 0.3925947601930365    Val Acc = 0.8019425675675675  Val Loss = 0.4248818360650602\n",
            "Epoch: 67/100   Train Acc = 0.831324027072758  Train Loss = 0.39035435447732514    Val Acc = 0.8019425675675675  Val Loss = 0.42473769544545503\n",
            "Epoch: 68/100   Train Acc = 0.8315355329949239  Train Loss = 0.3871467144049976    Val Acc = 0.8040540540540541  Val Loss = 0.4242520963435021\n",
            "Epoch: 69/100   Train Acc = 0.8342851099830795  Train Loss = 0.3860421326302402    Val Acc = 0.8061655405405406  Val Loss = 0.42797753288246326\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 70/100   Train Acc = 0.8367174280879864  Train Loss = 0.3834601630466997    Val Acc = 0.7913851351351351  Val Loss = 0.4339326949855719\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 71/100   Train Acc = 0.8347609983079526  Train Loss = 0.3808119551775552    Val Acc = 0.785472972972973  Val Loss = 0.4409377827946487\n",
            "EarlyStopping counter: 3 out of 20\n",
            "Epoch: 72/100   Train Acc = 0.8368231810490694  Train Loss = 0.37988432795969074    Val Acc = 0.7972972972972973  Val Loss = 0.42847923193628695\n",
            "EarlyStopping counter: 4 out of 20\n",
            "Epoch: 73/100   Train Acc = 0.8388324873096447  Train Loss = 0.3776397763649907    Val Acc = 0.809543918918919  Val Loss = 0.4224840319499973\n",
            "Epoch: 74/100   Train Acc = 0.8360300338409475  Train Loss = 0.37757257559711666    Val Acc = 0.7930743243243243  Val Loss = 0.42858533379172575\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 75/100   Train Acc = 0.839096869712352  Train Loss = 0.37750604561438234    Val Acc = 0.8002533783783784  Val Loss = 0.4286182129913238\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 76/100   Train Acc = 0.8395727580372251  Train Loss = 0.37463380022938847    Val Acc = 0.8023648648648649  Val Loss = 0.4220456698554072\n",
            "Epoch: 77/100   Train Acc = 0.8426395939086294  Train Loss = 0.3726116120118302    Val Acc = 0.7960304054054054  Val Loss = 0.4300364752850895\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 78/100   Train Acc = 0.8453891708967851  Train Loss = 0.36953665667107805    Val Acc = 0.8086993243243243  Val Loss = 0.42047253885265085\n",
            "Epoch: 79/100   Train Acc = 0.845019035532995  Train Loss = 0.3668125969803729    Val Acc = 0.8103885135135135  Val Loss = 0.4197877652487042\n",
            "Epoch: 80/100   Train Acc = 0.8449661590524534  Train Loss = 0.3638224990872455    Val Acc = 0.8032094594594594  Val Loss = 0.4236712530451442\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 81/100   Train Acc = 0.8451247884940778  Train Loss = 0.3626870845395927    Val Acc = 0.7994087837837838  Val Loss = 0.42467979935549516\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 82/100   Train Acc = 0.8484560067681896  Train Loss = 0.36295292422250386    Val Acc = 0.8053209459459459  Val Loss = 0.42027877927585755\n",
            "EarlyStopping counter: 3 out of 20\n",
            "Epoch: 83/100   Train Acc = 0.8474513536379019  Train Loss = 0.3591291445030906    Val Acc = 0.7795608108108109  Val Loss = 0.4545813718161448\n",
            "EarlyStopping counter: 4 out of 20\n",
            "Epoch: 84/100   Train Acc = 0.8467639593908629  Train Loss = 0.3595701679457621    Val Acc = 0.7913851351351351  Val Loss = 0.42739599874010015\n",
            "EarlyStopping counter: 5 out of 20\n",
            "Epoch: 85/100   Train Acc = 0.854431049069374  Train Loss = 0.3536810732070765    Val Acc = 0.8006756756756757  Val Loss = 0.4299904329315379\n",
            "EarlyStopping counter: 6 out of 20\n",
            "Epoch: 86/100   Train Acc = 0.8539551607445008  Train Loss = 0.3524956399249279    Val Acc = 0.8074324324324325  Val Loss = 0.4216782247019154\n",
            "EarlyStopping counter: 7 out of 20\n",
            "Epoch: 87/100   Train Acc = 0.8540609137055838  Train Loss = 0.35006610149066497    Val Acc = 0.8099662162162162  Val Loss = 0.42149943874698365\n",
            "EarlyStopping counter: 8 out of 20\n",
            "Epoch: 88/100   Train Acc = 0.8568633671742809  Train Loss = 0.3478291552185191    Val Acc = 0.8129222972972973  Val Loss = 0.42511734836168447\n",
            "EarlyStopping counter: 9 out of 20\n",
            "Epoch: 89/100   Train Acc = 0.855594331641286  Train Loss = 0.34618055398618464    Val Acc = 0.8057432432432432  Val Loss = 0.42183974683522313\n",
            "EarlyStopping counter: 10 out of 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cZpcC5sgQdh"
      },
      "source": [
        "Testing and plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8mayjmahduz"
      },
      "source": [
        "sp.call(['cp', 'checkpoint.pt', model_file])\n",
        "sp.call(['mv', model_file, result_path+model_file])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlmXY0jSAAzi"
      },
      "source": [
        "result = pd.DataFrame()\n",
        "result['train_loss'] = train_loss_list\n",
        "result['train_acc'] = train_acc_list\n",
        "result['val_loss'] = val_loss_list\n",
        "result['val_acc'] = val_acc_list\n",
        "result['early_stop'] = [early_stop_epoch for i in range(len(val_acc_list))]\n",
        "result.to_csv(result_path + train_history_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK6cquvgh60g"
      },
      "source": [
        "test_model = CustomDWT()\n",
        "test_model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "test_model = test_model.to(device)\n",
        "test_model.double()\n",
        "test_model = test_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVp2-5tmiI-S"
      },
      "source": [
        "test_params = {\n",
        "    'shuffle':False,\n",
        "    'batch_size': 1\n",
        "}\n",
        "test_data = PytorchDataGen(test_list)\n",
        "test_gen = DataLoader(test_data, **test_params)\n",
        "\n",
        "total_acc = []\n",
        "total_loss = []\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for x_test, y_test in test_gen:\n",
        "  \n",
        "  pred = test_model(x_test.to(device))\n",
        "  y_test = y_test.to(device)\n",
        "  #y_test = y_test.unsqueeze(1)\n",
        "  loss = criterion(pred, y_test)\n",
        "  acc = calc_acc(pred, y_test)\n",
        "\n",
        "  total_loss.append(loss.item())\n",
        "  total_acc.append(acc)\n",
        "  \n",
        "  softmax = torch.exp(pred).cpu()\n",
        "  prob = list(softmax.detach().numpy())\n",
        "  pred_label = np.argmax(prob, axis = 1)\n",
        "  y_test = y_test.detach().cpu().numpy()\n",
        "  y_pred.append(pred_label[0])\n",
        "  y_true.append(y_test[0])\n",
        "\n",
        "print(f\"Average accuracy {np.average(total_acc)}\")\n",
        "print(f\"Average loss {np.average(total_loss)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9ce_-QzrcKb"
      },
      "source": [
        "#Negative = interictal (0)\n",
        "#Positive = preictal (1)\n",
        "TN, FP, FN, TP = confusion_matrix(y_true, y_pred).ravel()\n",
        "sensitivity = TP/(TP + FN)\n",
        "specificity = TN/(TN + FP)\n",
        "fpr = FP/(FP + TN)\n",
        "print(f\"Sensitivity: {sensitivity}\")\n",
        "print(f\"Specificity: {specificity}\")\n",
        "print(f\"False Positive rate: {fpr}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfhQDtPHoMWb"
      },
      "source": [
        "print(f\"True Positive: {TP}\")\n",
        "print(f\"True Negative: {TN}\")\n",
        "print(f\"False Positive: {FP}\")\n",
        "print(f\"False Negative: {FN}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZXBv2omFphE"
      },
      "source": [
        "plt.plot(train_loss_list[0:early_stop_epoch], color = 'black')\n",
        "plt.plot(val_loss_list[0:early_stop_epoch], color = 'red')\n",
        "plt.plot(train_acc_list[0:early_stop_epoch], color = 'blue')\n",
        "plt.plot(val_acc_list[0:early_stop_epoch], color = 'green')\n",
        "plt.legend(['Train loss', 'Val loss', 'Train Acc', 'Val Acc', 'Early Stop'], loc = 'upper right')\n",
        "plt.savefig(result_path + plot_file, bbox_inches = \"tight\")\n",
        "plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy_AXYNRTotE"
      },
      "source": [
        "\n",
        "#logging data to file\n",
        "with open(result_path+log_file, \"w\") as f:\n",
        "  print(f\" Continuos Learning: {IS_CONT}\\n Window Size Limit: {WINDOW_SIZE_END if IS_CONT else 0}\\nLearning Rate: {LEARNING_RATE}\\nMomentum: {MOMENTUM}\\nL2: {L2}\\nSensitivity: {sensitivity}\\nSpecificity: {specificity}\\nFPR: {fpr}\\nTrue Positive: {TP}\\nTrue Negative: {TN}\\nFalse Positive: {FP}\\nFalse Negative: {FN}\", file = f)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrA9-lmkDTmu"
      },
      "source": [
        "##Old Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEa2ZkjjoeKW"
      },
      "source": [
        "class DWTLayer(Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(DWTLayer, self).__init__()\n",
        "    \n",
        "    self.linear_1 = Linear(1289, 512)\n",
        "    self.linear_2 = Linear(654, 512)\n",
        "    self.linear_3 = Linear(336, 512)\n",
        "    self.linear_4 = Linear(177, 512)\n",
        "  \n",
        "    self.batch_norm = BatchNorm2d(4)\n",
        "  def forward(self, x):\n",
        "    cd_1, cd_2, cd_3, cd_4 = list(), list(), list(), list()\n",
        "    x = x.cpu().detach().numpy()\n",
        "\n",
        "    #iterating on each sample inside a batch\n",
        "    for i in range(x.shape[0]): \n",
        "      item = x[i]\n",
        "      cd_1_temp, cd_2_temp, cd_3_temp, cd_4_temp = list(), list(), list(), list()\n",
        "\n",
        "      #iterating each channel\n",
        "      for ch in range(item.shape[0]):\n",
        "        single_ch = item[ch]\n",
        "        d_and_a = wavedec(single_ch, 'db10', level = 4)\n",
        "        cd_4_temp.append(d_and_a[1])\n",
        "        cd_3_temp.append(d_and_a[2])\n",
        "        cd_2_temp.append(d_and_a[3])\n",
        "        cd_1_temp.append(d_and_a[4])\n",
        "      \n",
        "      cd_1.append(cd_1_temp)\n",
        "      cd_2.append(cd_2_temp)\n",
        "      cd_3.append(cd_3_temp)\n",
        "      cd_4.append(cd_4_temp)\n",
        "      \n",
        "    cd_1 = np.array(cd_1)\n",
        "    cd_2 = np.array(cd_2)\n",
        "    cd_3 = np.array(cd_3)\n",
        "    cd_4 = np.array(cd_4)\n",
        "\n",
        "    cd_1 = torch.from_numpy(cd_1).to(device)\n",
        "    cd_2 = torch.from_numpy(cd_2).to(device)\n",
        "    cd_3 = torch.from_numpy(cd_3).to(device)\n",
        "    cd_4 = torch.from_numpy(cd_4).to(device)\n",
        "    \n",
        "    cd_1 = self.linear_1(cd_1)\n",
        "    cd_2 = self.linear_2(cd_2)\n",
        "    cd_3 = self.linear_3(cd_3)\n",
        "    cd_4 = self.linear_4(cd_4)\n",
        "          \n",
        "    x = torch.stack([cd_1, cd_2, cd_3, cd_4], dim = 1)        \n",
        "    x = self.batch_norm(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUMVw6RJokaA"
      },
      "source": [
        "#trainable parameter neural network\n",
        "class CustomDWT(Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CustomDWT, self).__init__()    \n",
        "    self.conv_1 = Conv2d(4, 4, kernel_size = (3,3), stride = (1, 2))\n",
        "    self.conv_2 = Conv2d(4, 4, kernel_size = (3,3), stride = (1, 2))\n",
        "    self.conv_3 = Conv2d(4, 16, kernel_size = (5,5), stride = (1, 2), padding = (1,0))\n",
        "    self.conv_4 = Conv2d(16, 16, kernel_size = (5,5), stride = (1, 2), padding = (1, 0))\n",
        "    \n",
        "    self.batch_norm_1 = BatchNorm2d(4)\n",
        "    self.batch_norm_2 = BatchNorm2d(16)\n",
        "\n",
        "    self.mpl = MaxPool2d(kernel_size = 2, stride = 2)\n",
        "   \n",
        "    self.linear_1 = Linear(624, 256)    \n",
        "    self.linear_2 = Linear(256, 128)\n",
        "    self.linear_3 = Linear(128, 2)\n",
        "    \n",
        "    self.batch_norm_l1 = BatchNorm1d(256)\n",
        "    self.batch_norm_l2 = BatchNorm1d(128)    \n",
        "\n",
        "    self.dwt_layer = DWTLayer()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #dwt layer\n",
        "    x = self.dwt_layer(x)\n",
        "    \n",
        "    #cnn blocks\n",
        "    x = torch.tanh(self.conv_1(x))    \n",
        "    x = torch.tanh(self.conv_2(x))\n",
        "    x = self.batch_norm_1(x)\n",
        "    x = self.mpl(x)\n",
        "    x = torch.tanh(self.conv_3(x))    \n",
        "    x = torch.tanh(self.conv_4(x))\n",
        "    x = self.batch_norm_2(x)\n",
        "\n",
        "    #faltening\n",
        "    x = x.view(x.size(0), -1)    \n",
        "\n",
        "    #FCNs    \n",
        "    x = self.linear_1(x)\n",
        "    x = self.batch_norm_l1(x)    \n",
        "    x = torch.tanh(x)\n",
        "\n",
        "    x = self.linear_2(x)\n",
        "    x = self.batch_norm_l2(x)\n",
        "    x = torch.tanh(x)\n",
        "\n",
        "    x = self.linear_3(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}